{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35baa466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycocotools\n",
    "\n",
    "from pycocotools.coco import COCO # COCO python library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, Attention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import add\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "# small library for seeing the progress of loops.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074df0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.29s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco=COCO(r\"D:\\SEM 5\\Sem Project\\annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60ca54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03718545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "annFile = r\"D:\\SEM 5\\Project Blind\\annotations\\captions_train2017.json\"\n",
    "coco_caps=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d752e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae4f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65833bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of main categories:  12\n",
      "List of main categories:  ['appliance', 'sports', 'furniture', 'animal', 'outdoor', 'food', 'electronic', 'accessory', 'kitchen', 'indoor', 'vehicle', 'person']\n"
     ]
    }
   ],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())\n",
    "maincategories = list(set([cat['supercategory'] for cat in cats]))\n",
    "\n",
    "print(\"Number of main categories: \", len(maincategories))\n",
    "print(\"List of main categories: \", maincategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acefab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub categories:  80\n",
      "List of sub categories:  ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "subcategories = [cat['name'] for cat in cats]\n",
    "\n",
    "print(\"Number of sub categories: \", len(subcategories))\n",
    "print(\"List of sub categories: \", subcategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7218dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub categories with IDs : {'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13, 'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17, 'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22, 'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27, 'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33, 'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37, 'kite': 38, 'baseball bat': 39, 'baseball glove': 40, 'skateboard': 41, 'surfboard': 42, 'tennis racket': 43, 'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48, 'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53, 'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57, 'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61, 'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65, 'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73, 'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77, 'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81, 'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86, 'scissors': 87, 'teddy bear': 88, 'hair drier': 89, 'toothbrush': 90}\n"
     ]
    }
   ],
   "source": [
    "catIds = coco.getCatIds(catNms=subcategories)\n",
    "\n",
    "subcategories_Ids = dict()\n",
    "for i in range(0,len(subcategories)):\n",
    "    subcategories_Ids[subcategories[i]] = catIds[i]\n",
    "\n",
    "print(\"Sub categories with IDs :\",subcategories_Ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda1e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub categories with Image IDs : 80\n"
     ]
    }
   ],
   "source": [
    "subcategories_imageIds = dict()\n",
    "\n",
    "for i in range(0,len(catIds)):\n",
    "    imgIds = coco.getImgIds(catIds=catIds[i])\n",
    "    img = []\n",
    "    for j in imgIds:\n",
    "        img.append(j)\n",
    "    subcategories_imageIds[subcategories[i]] = img\n",
    "\n",
    "print(\"Sub categories with Image IDs :\",len(subcategories_imageIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db043eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images:  6221\n"
     ]
    }
   ],
   "source": [
    "train_cats = subcategories_imageIds['bicycle'] + subcategories_imageIds['airplane']\n",
    "imgIdss = coco.getImgIds(imgIds = train_cats)\n",
    "print(\"Total Images: \", len(imgIdss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b4aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  6221\n",
      "['<start> a jumbo jet plane connected to a boarding deck <end>', '<start> a large blue passenger plane sits on the tarmac at the airport <end>', '<start> a blue commercial airplane parked at a jet way <end>', '<start> a large airplane that is sitting out on the runway <end>', '<start> a blue plane at the airport being offloaded <end>']\n"
     ]
    }
   ],
   "source": [
    "dataset = dict()\n",
    "imgcaptions = []\n",
    "\n",
    "for imgid in imgIdss:\n",
    "    img = coco.loadImgs(imgid)[0]\n",
    "    annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
    "    anns = coco_caps.loadAnns(annIds)\n",
    "    imgcaptions = []\n",
    "    for cap in anns:\n",
    "       \n",
    "        # Remove punctuation\n",
    "        cap = cap['caption'].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Replace - to blank\n",
    "        cap = cap.replace(\"-\",\" \")\n",
    "\n",
    "        # Split string into word list and Convert each word into lower case\n",
    "        cap = cap.split()\n",
    "        cap = [word.lower() for word in cap]\n",
    "\n",
    "        # join word list into sentence and <start> and <end> tag to each sentence which helps\n",
    "        # LSTM encoder-decoder model while training.\n",
    "\n",
    "        cap = '<start> ' + \" \".join(cap) + ' <end>'\n",
    "        imgcaptions.append(cap)\n",
    "\n",
    "    dataset[img['coco_url']] = imgcaptions\n",
    "\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "print(dataset['http://images.cocodataset.org/train2017/000000047084.jpg'])\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf327327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "flatten_list = list(chain.from_iterable(dataset.values())) \n",
    "tokenizer = Tokenizer(oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(flatten_list)\n",
    "max_length = 46\n",
    "\n",
    "\n",
    "model = load_model('models/mymodel7_0.h5')\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(filename, model):\n",
    "        try:\n",
    "            image = Image.open(filename)\n",
    "\n",
    "        except:\n",
    "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
    "        image = image.resize((299,299))\n",
    "        image = np.array(image)\n",
    "\n",
    "        # for images that has 4 channels, we convert them into 3 channels\n",
    "        if image.shape[2] == 4:\n",
    "            image = image[..., :3]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "        feature = model.predict(image)\n",
    "        return feature\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def caption_generator(img):\n",
    "    \n",
    "    xception_model = Xception(include_top=False, pooling=\"avg\")\n",
    "    photo = extract_features(img, xception_model)\n",
    "    in_text = 'start'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        pred = model.predict([photo,sequence], verbose=0)\n",
    "        pred = np.argmax(pred)\n",
    "        word = word_for_id(pred, tokenizer)\n",
    "\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f273e4",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import warnings,logging\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)\n",
    "from transformers import pipeline\n",
    "caption = pipeline('image-to-text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a09da1",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import warnings,logging\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)\n",
    "from transformers import pipeline\n",
    "caption = pipeline('image-to-text')\n",
    "def caption_generator(img_pil):\n",
    "    image_captions = caption(img_pil)\n",
    "    for captionss in image_captions:\n",
    "        for key, value in captionss.items():\n",
    "            return value\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee0a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "End_Phrases = [\"Goodbye\",\"Exit chat\",\"Stop chatting\",\"End conversation\",\"Close chat\",\"Finish chat\",\"Thank you\",\"I'm done\",\"Logout\",\"See you later\",\"Bye for now\",\"Shut down\",\"Quit chat\",\"Terminate conversation\",\"Farewell\", \"I'm finished\",\"Conclude chat\",\"Log off\",\"Shutdown chatbot\",\"Wrap it up\",\"Cease conversation\",\"End the session\",\"Sign out\",\"Thanks, bye\",\"Time to go\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c684e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer  =>  Hello Dear! I am Alexa ! How can I help you\n",
      "Please start speaking...\n",
      "Sorry, I couldn't understand what you said.\n",
      "Computer  =>  Cant hear you! Please Repeat\n",
      "cycle    1\n",
      "Please start speaking...\n",
      "Sorry, I couldn't understand what you said.\n",
      "Computer  =>  Cant hear you! Please Repeat\n",
      "cycle    1\n",
      "Please start speaking...\n",
      "You  =>  the output\n",
      "Computer  =>  Cant hear you! Please Repeat\n",
      "cycle    1\n",
      "Please start speaking...\n",
      "You  =>  start the camera\n",
      "Computer  =>  Cant hear you! Please Repeat\n",
      "cycle    1\n",
      "Please start speaking...\n",
      "You  =>  alexa start the camera\n",
      "Computer  =>  start a man with glasses and a beard  end\n",
      "Computer  =>  start a man with glasses and a scarf  end\n",
      "Computer  =>  start a man with glasses looking at himself in the mirror  end\n",
      "Computer  =>  start two men in glasses are looking at a mirror  end\n",
      "Computer  =>  start two men in glasses with a reflection of a man in a mirror  end\n",
      "Computer  =>  start two men in a mirror with a cell phone  end\n",
      "Computer  =>  start two men in a room with a mirror  end\n",
      "Computer  =>  start two men in a mirror with a toothbrush  end\n",
      "Computer  =>  start two men in a mirror with a toothbrush  end\n",
      "Computer  =>  start two men in a mirror with a reflection of themselves  end\n",
      "Computer  =>  start a man with glasses looking at himself in a mirror  end\n",
      "Computer  =>  start two men in a mirror with a reflection of themselves  end\n",
      "Computer  =>  start two men in a room with a mirror  end\n",
      "Computer  =>  start two men in a room with a mirror  end\n",
      "Computer  =>  start two men in glasses are looking at a mirror  end\n",
      "Computer  =>  start two men in a room with a mirror  end\n",
      "Computer  =>  start two men in a mirror with one of them holding a toothbrush  end\n",
      "Computer  =>  start two men in glasses looking at the camera  end\n",
      "Computer  =>  start a man with glasses and a beard  end\n",
      "Computer  =>  start a man with glasses and a beard is holding a piece of paper  end\n",
      "Computer  =>  start a man with glasses and a beard is looking at his phone  end\n",
      "Computer  =>  start a man with glasses and a beard is looking at a mirror  end\n",
      "Computer  =>  start two men in a mirror with a reflection of a man in the mirror  end\n",
      "Computer  =>  start a man with a beard is looking at himself in the mirror  end\n",
      "Computer  =>  start a man with a beard is standing in front of a wall  end\n",
      "Computer  =>  start a man with a beard is holding a paper  end\n",
      "Computer  =>  start a man with a beard is standing in front of a wall  end\n",
      "Computer  =>  start a man with glasses and a scarf  end\n",
      "Computer  =>  start a man with glasses looking in a mirror  end\n",
      "Stop ________________________:)()()((()()()))\n",
      "cycle    1\n",
      "Please start speaking...\n",
      "You  =>  alexa start the camera\n",
      "Computer  =>  start two men in glasses looking at their phones  end\n",
      "Computer  =>  start a man with glasses and a beard is looking at the camera  end\n",
      "Computer  =>  start two men in glasses are smiling and posing for a picture  end\n",
      "Computer  =>  start a man with glasses and a beard is looking at the camera  end\n",
      "Computer  =>  start two men in a room with a mirror  end\n",
      "Computer  =>  start a man with glasses and a beard  end\n",
      "Computer  =>  start a man with glasses and a beard  end\n",
      "Computer  =>  start a man with glasses and a beard looking at the camera  end\n",
      "Computer  =>  start a room with a wall with a light on  end\n",
      "Computer  =>  start a man in a dark room with a mirror  end\n",
      "Computer  =>  start a person standing in a room with a light on  end\n",
      "Computer  =>  start a dark room with a light on and a wall with a light  end\n",
      "Computer  =>  start a light shines through a wall in a dark room  end\n",
      "Computer  =>  start a man in a white shirt is looking at a computer  end\n",
      "Computer  =>  start a room with a ceiling fan and a ceiling fan  end\n",
      "Computer  =>  start a person standing in a room with a ceiling fan  end\n",
      "Computer  =>  start a man with a beard and a tie  end\n",
      "Computer  =>  start a man in a black shirt and black pants  end\n",
      "Computer  =>  start a man in a hat and tie  end\n",
      "Computer  =>  start a man with glasses and a beard is looking at a camera  end\n",
      "Stop ________________________:)()()((()()()))\n",
      "cycle    1\n",
      "Please start speaking...\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import pywhatkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import datetime\n",
    "import requests,json\n",
    "from urllib.request import urlopen\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty(\"voices\")\n",
    "engine.setProperty(\"voice\" , voices[1].id)\n",
    "\n",
    "greeting=[\"hello\",\"hi\",\"whats up\"]\n",
    "\n",
    "def engine_talk(text):\n",
    "    print(f\"Computer  =>  {text}\")\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def speak():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Please start speaking...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = recognizer.listen(source)\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            command = text.lower()\n",
    "            print(f\"You  =>  {command}\")\n",
    "            if(\"alexa\" in command):\n",
    "                return command;\n",
    "            \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I couldn't understand what you said.\")\n",
    "\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "def weather():\n",
    "\n",
    "    url = 'http://ipinfo.io/json'\n",
    "    response = urlopen(url)\n",
    "    data = json.load(response)\n",
    "\n",
    "    api_key = '7082eafcabdc1749014c8a9c2b298ffd'\n",
    "    city_name = data['city']\n",
    "    print(city_name)\n",
    "    endpoint = f'http://api.weatherstack.com/current?access_key={api_key}&query={city_name}'\n",
    "    response = requests.get(endpoint)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        return [ data[\"current\"][\"weather_descriptions\"][0], data[\"current\"]['temperature'] ]\n",
    "    else:\n",
    "        print(f'Error: Unable to fetch weather data. Status code: {response.status_code}')\n",
    "\n",
    "\n",
    "def run_alexa():\n",
    "    command=speak()\n",
    "    if(command == None):\n",
    "        engine_talk(\"Cant hear you! Please Repeat\" )\n",
    "        return 1\n",
    "\n",
    "    elif (\"play\" in command):\n",
    "        stop_words=stopwords.words('english')\n",
    "        stop_words.append(\"alexa\")\n",
    "        stop_words.append(\"play\")\n",
    "        stop_words.append(\"hello\")\n",
    "        arr=word_tokenize(command)\n",
    "        res=[word for word in arr if word not in stop_words]\n",
    "        command = ' '.join(res)\n",
    "        print(\"You said:\", command)\n",
    "        pywhatkit.playonyt(command)\n",
    "        engine_talk(\"Playing \" + command)\n",
    "    elif(\"date\" in command):\n",
    "        now = datetime.datetime.now()\n",
    "        day= now.date().day\n",
    "        month_map = {\n",
    "            1: \"January\",\n",
    "            2: \"February\",\n",
    "            3: \"March\",\n",
    "            4: \"April\",\n",
    "            5: \"May\",\n",
    "            6: \"June\",\n",
    "            7: \"July\",\n",
    "            8: \"August\",\n",
    "            9: \"September\",\n",
    "            10: \"October\",\n",
    "            11: \"November\",\n",
    "            12: \"December\",\n",
    "        }\n",
    "        month= now.date().month\n",
    "        year= now.date().year\n",
    "        engine_talk(f\"Today date is {day} {month_map[month]} {year}\")\n",
    "        \n",
    "    elif(\"time\" in command):\n",
    "        now = datetime.datetime.now()\n",
    "        hour= now.time().hour\n",
    "        minute= now.time().minute\n",
    "        if(hour > 12):\n",
    "            engine_talk(f\"Time  is {hour%12} {minute} pm\")\n",
    "        else:\n",
    "            engine_talk(f\"The Current Time is {hour%12}:{minute} am\")\n",
    "    elif(\"weather\" in command or \"temperature\" in command ) :\n",
    "        cond,temp = weather()\n",
    "        print(f\"Climate is {cond} in your city with temperature {temp}\")\n",
    "        engine_talk(f\"Climate is {cond} in your city with temperature {temp}\")\n",
    "    \n",
    "    elif(\"start\" in command):\n",
    "        \n",
    "        capt = cv2.VideoCapture(0) \n",
    "        counter=0\n",
    "\n",
    "        while True:\n",
    "            # Read a frame from the camera\n",
    "            ret, frame = capt.read()\n",
    "\n",
    "            # Check if the frame was successfully read\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if(counter%25==0):\n",
    "                description = \"start \" + caption_generator(img_pil) + \" end\"\n",
    "                engine_talk(description)\n",
    "\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "\n",
    "            img_cv2 = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            cv2.putText(img_cv2, description, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow('Mobile Camera', img_cv2)\n",
    "\n",
    "            # Exit loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "                print(\"Stop ________________________:)()()((()()()))\")\n",
    "                engine.stop()\n",
    "                break\n",
    "\n",
    "        # Release the VideoCapture and close the OpenCV windows\n",
    "        capt.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "    elif(\"quit\" in command or \"exit\" in command or \"bye\" in command):\n",
    "        engine_talk(\"Thank You its a lovely interaction\")   \n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        print(\"I Could not hear you properly\")\n",
    "    return 1\n",
    "\n",
    "engine_talk(\"Hello Dear! I am Alexa ! How can I help you\")\n",
    "cycle=1\n",
    "while(cycle):\n",
    "    cycle = run_alexa()\n",
    "    print(\"cycle   \",cycle)\n",
    "# weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e53f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "capt = cv2.VideoCapture(0) \n",
    "capt.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
